(* Wolfram Language Package *)

BeginPackage["TelegramBotAPI`xAI`"]

Unprotect[Evaluate[$Context<>"*"]];


(* Exported symbols added here with SymbolName::usage *)
(* Usage messages: use here String Representation of Boxes to get formatting similar to built-in functions *)
(* Functions*)
chatCompletionXai::usage = "chatCompletionXai[\!\(\*RowBox[{StyleBox[\"messages\",\"TI\"],\",\" , StyleBox[\"model\",\"TI\"]}]\)] returns a \!\(\*StyleBox[\"model\",\"TI\"]\) response for the chat conversation given as \!\(\*StyleBox[\"messages\",\"TI\"]\) list.
chatCompletionXai[\!\(\*RowBox[{StyleBox[\"messages\",\"TI\"],\",\" , StyleBox[\"model\",\"TI\"],\",\" , StyleBox[\"options\",\"TI\"]}]\)] returns the response applying specified \!\(\*StyleBox[\"options\",\"TI\"]\) given as a sequence of rules.
chatCompletionXai[\!\(\*RowBox[{\"\[Ellipsis]\", \",\" , \"\\\"frequency_penalty''\", \"\[Rule]\", StyleBox[\"value\",\"TI\"]}]\)] returns the response applying the specified \!\(\*StyleBox[\"value\",\"TI\"]\) to the chosen option. N.B. Option names are given as strings.
The options to choose from:
 \"frequency_penalty\": Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim; defaults to 0.
 \"logit_bias\": A JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token; defaults to Null.
 \"logprobs\": Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message; defaults to False.
 \"max_tokens\": The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API; defaults to Null.
 \"n\": How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs; defaults to 1.
 \"presence_penalty\": Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics; defaults to 0.
 \"response_format\": An object specifying the format that the model must output; defaults to Null. 
 \"seed\": If specified, our system will make a best effort to sample deterministically; defaults to Null. 
 \"stop\": Up to 4 sequences where the API will stop generating further tokens; defaults to Null.
 \"stream\": If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a \"DONE\" message; defaults to False.
 \"stream_options\": Options for streaming response. Only set this when you set \"stream\" \[Rule] True; defaults to Null.
 \"temperature\":  What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic; defaults to 1.
 \"tool_choice\":  Controls which (if any) tool is called by the model. \"none\" means the model will not call any tool and instead generates a message. \"auto\" means the model can pick between generating a message or calling one or more tools. \"required\" means the model must call one or more tools; defaults to \"none\".
 \"tools\": A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported; defaults to Null. 
 \"top_logprobs\": An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. \"logprobs\" must be set to True if this parameter is used; defaults to Null.
 \"top_p\": An alternative to sampling with \"temperature\". It is generally recommended to alter this or \"temperature\" but not both; defaults to 1.
 \"user\": A unique identifier representing your end-user, which can help xAI to monitor and detect abuse; defaults to \"\".  
 \"APIKey\": Your \!\(\*TemplateBox[{\"authentication key\", \"https://docs.x.ai/docs/tutorial#creating-an-api-key\"}, \"HyperlinkURL\"]\) to access xAI API; defaults to $xAIAPIKey.
\!\(\*TemplateBox[{\"Read on xAI API webpage\", \"https://docs.x.ai/docs/guides/chat#parameters\"}, \"HyperlinkURL\"]\)";




(* Options *)

(* Constants *) 
$XaiURL::usage = "URL for all queries to the xAI API.
\!\(\*TemplateBox[{\"Read on xAI API webpage\", \"https://docs.x.ai/docs/introduction#grok-on-x-vs-xai-api\"}, \"HyperlinkURL\"]\)"
$XaiAPIKey::usage = "Key used for authentication during API requests to xAI.
\!\(\*TemplateBox[{\"Read on xAI API webpage\", \"https://docs.x.ai/docs/tutorial#creating-an-api-key\"}, \"HyperlinkURL\"]\)";
 

Begin["`Private`"] (* Begin Private Context *) 


Clear[$XaiURL,$XaiAPIKey]
(*
$XaiURL, $XaiAPIKey and etc. are effectively set to themselves 
so that they can enter all the package functions. Then they can be changed during the session 
and take an effect on all the functions, where they are used. 
*)


chatCompletionXai::connect = "A message was generated while connecting to `1`.";
chatCompletionXai::err = "An error response `1` has been received from `2`.";
Options[chatCompletionXai] = {
   "frequency_penalty" -> 0,
   "logit_bias" -> Null,
   "logprobs" -> False,
   "max_tokens" -> Null,
   "n" -> 1,
   "presence_penalty" -> 0,
   "response_format" -> Null,
   "seed" -> Null,
   "stop" -> Null,
   "stream" -> False,
   "stream_options" -> Null,
   "temperature" -> 1,
   "tool_choice" -> "none",
   "tools" -> Null,
   "top_logprobs" -> Null,
   "top_p" -> 1,
   "user" -> "",
   
   "APIKey" -> $XaiAPIKey
   };
chatCompletionXai[messages_?ListQ, model_?StringQ, OptionsPattern[]] := Catch[Block[
    {
     tools = OptionValue["tools"],
         
     apiurl, assoc, request, result
     },
    
    apiurl = StringJoin[$XaiURL, "chat/completions"];
    assoc = Association[
      Method -> "POST",
      "Headers" -> <|"Authorization" -> "Bearer " <> OptionValue["APIKey"]|>,
      "ContentType" -> "application/json",
      "Body" -> ExportString[{
         "messages" -> messages,
         "model" -> model,
         "frequency_penalty" -> OptionValue["frequency_penalty"],
         "logit_bias" -> OptionValue["logit_bias"],
         "logprobs" -> OptionValue["logprobs"],
         "max_tokens" -> OptionValue["max_tokens"],
         "n" -> OptionValue["n"],
         "presence_penalty" -> OptionValue["presence_penalty"],
         "response_format" -> OptionValue["response_format"],
         "seed" -> OptionValue["seed"],
         "stop" -> OptionValue["stop"],
         "stream" -> OptionValue["stream"],
         "stream_options" -> OptionValue["stream_options"],
         "temperature" -> OptionValue["temperature"],
         Sequence @@ If[
           tools =!= Null,
           {
           	"tools" -> tools,
            "tool_choice" -> OptionValue["tool_choice"]
            },
           {"tools" -> tools}
           ],
         "top_logprobs" -> OptionValue["top_logprobs"],
         "top_p" -> OptionValue["top_p"],
         "user" -> OptionValue["user"]
         }, "JSON"]];
    
    request = Check[URLExecute[HTTPRequest[apiurl, assoc]], {"created" -> Null}];
    
    If[
     	Lookup[request, "created"] === Null,
     	Message[chatCompletionXai::connect, $XaiURL];
     	Throw[{"Connection problem...", $Failed}],
     	result = Lookup[request, "error"];	
     	If[
      		MissingQ[result],
      		request,
      		Message[chatCompletionXai::err, result, $XaiURL];
      		Throw[{Lookup[result, "message"], $Failed}]
      	]
     ]
    ](* end Block *)](* end Catch*);
SyntaxInformation[chatCompletionXai] = {"ArgumentsPattern" -> {_,_,OptionsPattern[]}};



(* access to xAI api functions *)
$XaiURL = "https://api.x.ai/v1/";
(* after the package constants have entered all the functions they can be set to empy strings that will be 
their initial values *)
$XaiAPIKey = "";

End[] (* End Private Context *)


(Attributes[#] = {Protected, ReadProtected}) & /@ Complement[Names[Evaluate[$Context<>"*"]], {"$XaiURL","$XaiAPIKey"}]


EndPackage[]